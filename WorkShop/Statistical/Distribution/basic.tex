% -------------------------------------------------
% Settings
\documentclass[a4paper]{article}

\usepackage{amssymb}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}

\title{Basic of Distribution}
\author{listenzcc}

\begin{document}
% ------------------------------------------------
% Title page

\maketitle

% --------------------------------------------------
% Abstract
\abstract
A family of \emph{normal} distributions, like
Normal, Chi-squared and Student's t-distribution.
The Normal Distribution is the \emph{core} conception,
all others are derived from it.

This article begins with Gamma and Beta function.
Since they are useful for computing the \emph{moments} the normal distribution family.

Then the distributions are described one by one.

The Appendix provides some necessary proofs.


% -----------------------------------------------------
% Table of contents
\newpage
\tableofcontents

\section{Pre-knowledge}
% -------------------------------------------------------
% Pre knowledge section

\subsection{Gamma and Beta function}

An infinity integral is called as \emph{Gamma ($\Gamma$) function}

\begin{equation}
    \Gamma(z) = \int_{0}^{\infty} x^{z-1} e^{-x} \,\mathrm{d}x
\end{equation}

The \emph{Beta ($B$) function} is a two-factor function, derived from $\Gamma$ function

\begin{equation}
    B(\alpha, \beta) = \frac{\Gamma(\alpha) \cdot \Gamma(\beta)}{\Gamma(\alpha + \beta)}
\end{equation}

\subsection{Important equations}

\begin{proposition} \label{Gamma function propositions}
    Some very important equations.

    The value of $\Gamma(\frac{1}{2})$
    \begin{equation*}
        \Gamma(\frac{1}{2}) = \sqrt {\pi}
    \end{equation*}

    The recursive of $\Gamma(n)$, the general situation,
    \begin{equation*}
        \begin{aligned}
            \Gamma(1+z) & = z \Gamma(z)   \\
            \Gamma(1-z) & = -z \Gamma(-z)
        \end{aligned}
    \end{equation*}

    The integer situation,
    \begin{equation*}
        \Gamma(n) = (n-1)! \quad \forall n \in \mathcal{N}^+
    \end{equation*}

    The relationship between $\Gamma$ and $e^{-x^{2}}$
    \begin{equation*}
        \Gamma(z) = 2 \int_{0}^{\infty} x^{2z-1} e^{-x^{2}} \,\mathrm{d}x
    \end{equation*}

    The relationship between $\Gamma$ and $B$ Function
    \begin{equation*}
        B(\alpha, \beta) = \int_{0}^{1} t^{\alpha-1} (1-t)^{\beta-1} \,\mathrm{d}t
    \end{equation*}

    See \ref{The relationship between Gamma and Beta} for proof.

\end{proposition}

\section{Normal distribution}
% ---------------------------------------------------------
% Normal distribution section

\subsection{Definition}

It is hard to say normal distribution is what, since almost every thing follows it.

The Probability Distribution Function \emph{(pdf)} of normal distribution is
\begin{equation}
    p(x) = \frac{1}{\sqrt{2\pi}\delta} \exp({-\frac{(x-\mu)^2}{2\delta^2}}), -\infty < x < \infty
\end{equation}

the symbolic notion is $p(x) \sim N(\mu, \delta^2)$.
When $\mu = 0$ and $\delta^2 = 1$, it is called standard normal distribution.

\subsection{Mean and Variance}

The mean and variance of the normal distribution is
\begin{equation*}
    \begin{aligned}
        Mean     & \triangleq E(x) = \mu                 \\
        Variance & \triangleq E(x^2) - E^2(x) = \delta^2
    \end{aligned}
\end{equation*}
it is easy to proof using Proposition \ref{Gamma function propositions}.

\section{Chi-squared distribution}
% ----------------------------------------------------------
% Chi-squared distribution section

\subsection{Definition}

If $Y_i \sim N(0, 1)$, then
\begin{equation}
    \chi^2 \equiv \sum_{i = 1}^{r} Y_i^2
\end{equation}
is distributed as Chi-squared \emph{$\chi^2$} distribution with $r$ degrees of freedom.
The symbolic notion is $p_r(x) \sim \chi^2(r)$.

The pdf of Chi-squared distribution is
\begin{equation}
    P_r(x) = \frac{x^{r/2-1} e^{-x/2}}{\Gamma(r/2) 2^{r/2}}, 0 < x < \infty
\end{equation}

\subsection{Mean and Variance}
The mean and variance of the chi-squared distribution is
\begin{equation*}
    \begin{aligned}
        Mean     & \triangleq E(x) = r             \\
        Variance & \triangleq E(x^2) - E^2(x) = 2r
    \end{aligned}
\end{equation*}
it is easy to proof using Proposition \ref{Gamma function propositions}.

\subsection{Relationship with Normal Distribution}

The Chi-squared distribution is derived from Normal Distribution.

\section{Student's t-distribution}
% ------------------------------------------------------------
% Student's t-distribution section

\subsection{Definition}

The probability distribution of a random variable $T$, of the form
\begin{equation*}
    T = \frac{\bar{x} - m}{s / \sqrt{N}}
\end{equation*}
where $\bar{x}$ is the sample mean value of all $N$ samples, $m$ is the population mean value and $s$ is the population standard deviation.

Or, in a more formal one
\begin{equation}
    T = \frac{X}{\sqrt{Y/r}}
\end{equation}
where $X \sim N(0, 1)$ and $Y \sim \chi_r^2$.

The pdf of Student's t-distribution is
\begin{equation}
    t_r(x) = \frac{\Gamma(\frac{r+1}{2})}{\Gamma(\frac{r}{2}) \sqrt{r\pi}} (1+\frac{x^2}{r})^{-\frac{r+1}{2}}, -\infty < x < \infty
\end{equation}
it is easy to proof the pdf is a pdf using \ref{The pdf of Student's t-distribution is a pdf}.

The pdf of Student's t-distribution can be computed using \ref{Compute the pdf of Student's t-distribution}.

\subsection{Relationship with Normal Distribution}

It is easy to see that $\lim_{r \to \infty} t_r(x) \sim N(0, 1)$.
It demonstrates that when $r$ is large enough, the Student's t-distribution is equalize to Normal Distribution.

\subsection{Mean and Variance}

The mean and variance of the Student's t-distribution is
\begin{equation*}
    \begin{aligned}
        Mean     & \triangleq E(x) = 0                        \\
        Variance & \triangleq E(x^2) - E^2(x) = \frac{r}{r-2}
    \end{aligned}
\end{equation*}

\appendix

\section{Appendix}
% -----------------------------------------------------
% Appendix

\subsection{The relationship between $\Gamma$ and $B(\alpha, \beta)$}

It is essential to know that
\begin{equation*}
    \Gamma(m)\Gamma(n) = B(m, n) \Gamma(m+n)
\end{equation*}

\begin{proof} \label{The relationship between Gamma and Beta}
    One can write
    \begin{equation*}
        \Gamma(m)\Gamma(n) = \int_{0}^{\infty} x^{m-1} e^{-x} dx \int_{0}^{\infty} y^{n-1} e^{-y} dy
    \end{equation*}

    Then rewrite it as a double integral
    \begin{equation*}
        \Gamma(m)\Gamma(n) = \int_{0}^{\infty} \int_{0}^{\infty} x^{m-1} y^{n-1} e^{-x-y} dx dy
    \end{equation*}

    Applying the substitution $x=vt$ and $y=v(1-t)$, we have
    \begin{equation*}
        \Gamma(m)\Gamma(n) = \int_{0}^{1} t^{m-1} (1-t)^{n-1} dt \int_{0}^{\infty} v^{m+n-1} e^{-v} dv
    \end{equation*}

    Using the definitions of $\Gamma$ and Beta functions, we have
    \begin{equation*}
        \Gamma(m)\Gamma(n) = B(m, n) \Gamma(m+n)
    \end{equation*}

    Hence proved.

\end{proof}

\subsection{The pdf of Student's t-distribution is a pdf}

The values of $t_r(x)$ is positive and the integral is $1$.
\begin{equation*}
    \int_{-\infty}^{\infty} t_r(x) \,\mathrm{d}x = 1
\end{equation*}

\begin{proof} \label{The pdf of Student's t-distribution is a pdf}
    Consider the variable part of Student's t-distribution
    \begin{equation*}
        f(x) = (1+\frac{x^2}{r})^{-\frac{r+1}{2}}, -\infty < x < \infty
    \end{equation*}

    use a replacement as following
    \begin{equation*}
        x^2 = \frac{y}{1-y}
    \end{equation*}
    it is easy to see that $\lim_{y \to 0} x = 0$ and $\lim_{y \to 1} x = \infty$.
    Additionally, the $x^2$ is even function.
    Thus we can write the integral of $f(x)$
    \begin{equation*}
        \int_{-\infty}^{\infty} f(x) \,\mathrm{d}x =
        2 \sqrt{r} \int_{0}^{1} (\frac{1}{1-y})^{-\frac{r+1}{2}} \,\mathrm{d} (\frac{y}{1-y})^\frac{1}{2}
    \end{equation*}
    it is not hard to find out that the integral may end up with
    \begin{equation*}
        \sqrt{r} \int_{0}^{1} (1-y)^{\frac{r}{2}-1} y^{\frac{1}{2}-1} \,\mathrm{d}y =
        \sqrt{r} B(\frac{r}{2}, \frac{1}{2})
    \end{equation*}

    Finally the normalization factor has to be
    \begin{equation*}
        \frac{\Gamma(\frac{r+1}{2})}{\sqrt{r} \Gamma(\frac{r}{2}) \Gamma(\frac{1}{2})}
    \end{equation*}
    which makes the integral of $t_r(x)$ is $1$.

\end{proof}

\subsection{Compute the pdf of Student's t-distribution}

Here, we provide a simple computation of the pdf of the Student's t-distribution.
\begin{equation*}
    T=\frac{X}{\sqrt{Y/r}}
\end{equation*}
in which $X \sim N(0, 1)$ and $Y \sim \chi^2(r)$, and they are independent.
Thus, we have
\begin{equation*}
    \begin{aligned}
        p(x) & \propto e^{-x^2/2}               \\
        p(y) & \propto y^{r/2-1} \cdot e^{-y/2}
    \end{aligned}
\end{equation*}

The random variable $t$ follows the equation $t=\frac{x}{\sqrt{y/r}}$.
Since we want to proof that
\begin{equation*}
    p(t) \propto (1+\frac{t^2}{r})^{-\frac{r+1}{2}}
\end{equation*}

\begin{proof} \label{Compute the pdf of Student's t-distribution}
    The joint probability of $p(x, y)$ matches
    \begin{equation*}
        p(x, y) \propto e^{-x^2/2} \cdot y^{r/2-1} \cdot e^{-y/2}
    \end{equation*}

    And the divergence of $p(x, y)$ is $p(x, y) dx dy$.
    We can use the variable replacement of
    \begin{equation*}
        \begin{aligned}
            y             & = \frac{x^2}{t^2} \cdot r \\
            \frac{dy}{dt} & \propto \frac{x^2}{t^3}
        \end{aligned}
    \end{equation*}

    Thus we have the joint probability of $p(x, t)$ matches
    \begin{equation*}
        p(x, t) \propto e^{-x^2/2} \cdot (\frac{x^2}{t^2})^{r/2-1} \cdot e^{-\frac{x^2}{2t^2}r} \cdot \frac{x^2}{t^3}
    \end{equation*}

    The probability of $p(t)$ can be expressed as
    \begin{equation*}
        p(t) \propto \int_{x} p(x, t) dx
    \end{equation*}

    Analysis the expression, we have
    \begin{equation*}
        \begin{aligned}
            p(t) & \propto t^{-r-1} \int_{x} x^{r} \cdot e^{-\frac{1}{2}(1+\frac{r}{t^2})x^2} dx           \\
            p(t) & \propto t^{-r-1} \cdot (1+\frac{r}{t^2})^\frac{-r-1}{2} \int_{z} z^{r} \cdot e^{z^2} dz \\
            p(t) & \propto (t^2 + r) ^ {-\frac{r+1}{2}}                                                    \\
            p(t) & \propto (1+\frac{t^2}{r})^{-\frac{r+1}{2}}
        \end{aligned}
    \end{equation*}

    The process uses the integral of $\Gamma$ function is constant, and $r$ is constant.
\end{proof}

After that, combining with the \ref{The pdf of Student's t-distribution is a pdf}, we should finally have the pdf function.

\end{document}