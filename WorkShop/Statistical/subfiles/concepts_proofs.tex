\documentclass[../main.tex]{subfiles}

\begin{document}
\section{Proofs of Concepts Section}

\subsection{Proof of Lemma\ref{Lemma: Relationship between expectation and variance}}
Prove that, the relationship between expectation and variance can be found as following
\begin{equation*}
    \mathcal{V} = \mathcal{E}(X^2) - \mathcal{E}^2(X)
\end{equation*}

\begin{proof}
    Compute the square in \eqref{Definition: Expectation and variance definition},
    we have
    \begin{align*}
        \mathcal{V} & = \int (x^2 - 2 x \mathcal{E} + \mathcal{E}^2) p(x) dx \\
                    & = \mathcal{E}(X^2) - \mathcal{E}^2(X)
    \end{align*}
    where $\mathcal{E}$ refers $\mathcal{E}(X)$.
    And, the equation uses the condition that the $\mathcal{E}$ is constant in the integral.
\end{proof}


\subsection{Proof of Lemma\ref{Lemma: Second-order mixing moment of independent variables}}
Prove that, if the statistics of $X$ and $Y$ are independent, then
\begin{equation*}
    Cov(X, Y) =
    \mathcal{E}(X, Y) - \mathcal{E}(X) \mathcal{E}(Y) = 0,
    \forall X \perp Y
\end{equation*}

\begin{proof}
    The independency guarantees
    \begin{align*}
        p(x|y)  & = p(x)            \\
        p(y|x)  & = p(y)            \\
        p(x, y) & = p(x) \cdot p(y)
    \end{align*}
    Using the definition of expectation in \eqref{Definition: Expectation and variance definition},
    we have $\mathcal{E}(X, Y) = \mathcal{E}(X) \cdot \mathcal{E}(Y)$.
\end{proof}


\subsection{Proof of Lemma\ref{Lemma: Variance of mean value}}
Prove that, the variance of mean value equals to the mean of all covariance terms
\begin{equation*}
    \mathcal{V}(\overline{X}) =
    \frac{1}{n^2} \sum_{i, j} Cov(X_i, X_j),
    i, j \in 1, 2, \dots, n
\end{equation*}
no matter the relationship between the statistics of $X_1, X_2, \dots, X_n$.

\begin{proof}
    The mean value of $n$ statistics can be expressed as
    \begin{equation*}
        \overline{X} = \frac{1}{n} \sum_{i} X_i,
        i \in 1, 2, \dots, n
    \end{equation*}

    Based on the definition, the variance can be expressed as
    \begin{align*}
        Var(\overline{X}) & =  \int_{X_1, X_2, \dots, X_n} {\overline{x}}^2 p(x_1, x_2, \dots, x_n) dx_1 x_2 \dots x_n \\
                          & - (\int_{X_1, X_2, \dots, X_n} \overline{x} p(x_1, x_2, \dots, x_n) dx_1 x_2 \dots x_n)^2  \\
                          & , i, j \in 1, 2, \dots, n
    \end{align*}

    Use the property of full probability rules, we have
    \begin{align*}
         & \int_{X_k, \dots} f(x_k) \cdot p(x_k, \dots) dx_k \dots                    \\
         & = \int_{X_k} f(x_k) \cdot p(x_k) dx_k                                      \\
         & \int_{X_i, X_j, \dots} f(x_i, x_j) \cdot p(x_i, x_j, \dots) dx_i x_j \dots \\
         & = \int_{X_i, X_j} f(x_i, x_j) \cdot p(x_i, x_j) dx_i x_j
    \end{align*}

    Thus the variance of the mean value can be formulated as
    \begin{align*}
        n^2 \cdot Var(\overline{X}) & = \sum_i \int x_i^2 p(x_i) dx_i + \sum_{i \neq j} \int x_i x_j p(x_i, x_j) dx_i x_j \\
                                    & - (\sum_i \int x_i p(x_i) dx_i)^2
    \end{align*}
    since the positive and negative terms are both of $n^2$ terms.
    By pairing them one-by-one, we come to
    \begin{align*}
        n^2 \cdot Var(\overline{X}) & = \sum_{i, j} \mathcal{E}(X_i X_j) - \mathcal{E}(X_i)\mathcal{E}(X_j) \\
        Var(\overline{X})           & = \frac{1}{n^2} \sum_{i, j} Cov(X_i, X_j)
    \end{align*}

    Hence proved.
\end{proof}

\end{document}