\documentclass[../main.tex]{subfiles}

\begin{document}
\subsection{Random Variables}
A variable is \emph{random} means it is not fixed.
It turns out that one can obtain different values every time.
The reason behind can be systemic or arbitrary.
The aim of statistical analysis is to uncover the reason, however it usually matters little during the calculation.
But the analysis can be valid only if the \emph{random variable follows certain rules} instead of being totally unreasonable.

\subsection{Statistics and Distribution}
To understand the rules, the obtained values should be calculated carefully to formulate \emph{new meaningful values}.
The new values are called \emph{statistics}.
The statistics are asserted to be following some certain \emph{distribution}.
The distribution refers to the rule that controls the uncertainty of the random variable.
A classic distribution contains two parts:
\begin{itemize}
    \item Values $x$: The possible values of the statistic
    \item Probabilities $p(x)$: The probabilities of the values
\end{itemize}
It is also intrinsic that the sum of the probabilities should be equal to ONE, no more no less.
\begin{equation*}
    \int p(x) = 1, \forall p(x) \in (0, 1)
\end{equation*}
The function of $p(x)$ is called \emph{probability distribution function (PDF)}.

\subsection{Statistics}
There are several commonly used statistics like: \emph{expectation}, \emph{variance}, and \textit{etc}.
\begin{itemize}
    \item Expectation: The expectation value of every good obtain, expressed as the first-order zero moment
    \item Variance: The variance of the statistics, expressed as the second-order center moment
\end{itemize}
\begin{equation}
    \label{Definition: Expectation and variance definition}
    \begin{split}
        Expectation = \mathcal{E} & = \int x \cdot p(x) dx                 \\
        Variance = \mathcal{V}    & = \int (x-\mathcal{E})^2 \cdot p(x) dx
    \end{split}
\end{equation}
\begin{lemma}
    \label{Lemma: Relationship between expectation and variance}
    For simplicity, the relationship between expectation and variance can be found following
    \begin{equation*}
        \mathcal{V} = \mathcal{E}(X^2) - \mathcal{E}^2(X)
    \end{equation*}
    \begin{proof}
        Compute the square in \eqref{Definition: Expectation and variance definition},
        we have
        \begin{align*}
            \mathcal{V} & = \int (x^2 - 2 x \mathcal{E} + \mathcal{E}^2) p(x) dx \\
                        & = \mathcal{E}(X^2) - \mathcal{E}^2(X)
        \end{align*}
        where $\mathcal{E}$ refers $\mathcal{E}(X)$.
        And, the equation uses the condition that the $\mathcal{E}$ is constant in the integral.
    \end{proof}
\end{lemma}

\subsubsection{Independency of statistics}
The distribution of \emph{two random variables} can be computed using \emph{joint probability} and \emph{conditional probability}.
\begin{equation*}
    p(x, y) = p(x) \cdot p(y|x) = p(y) \cdot p(x|y)
\end{equation*}
And the second-order moment of the two random variables is
\begin{equation*}
    \mathcal{E}(X, Y) = \iint x \cdot y \cdot p(x, y) dx dy
\end{equation*}

\bigbreak
\textbf{Independent Situation}

The simplest situation is the variables of $x$ and $y$ are independent with each other.
\begin{lemma}
    \label{Lemma: Mixed second-order moment of independent variables}
    If $x$ and $y$ are independent, then
    \begin{equation*}
        Cov(X, Y) =
        \mathcal{E}(X, Y) - \mathcal{E}(X) \mathcal{E}(Y) = 0,
        \forall X \perp Y
    \end{equation*}
    \begin{proof}
        The independency guarantees
        \begin{align*}
            p(x|y)  & = p(x)            \\
            p(y|x)  & = p(y)            \\
            p(x, y) & = p(x) \cdot p(y)
        \end{align*}
        Using the definition of expectation in \eqref{Definition: Expectation and variance definition},
        we have $\mathcal{E}(X, Y) = \mathcal{E}(X) \cdot \mathcal{E}(Y)$.
    \end{proof}
\end{lemma}

\bigbreak
\textbf{Non-independent Situation}

If the independent situation is not matched, then the covariance is not zero.
\begin{equation*}
    Cov(X, Y) =
    \mathcal{E}(X, Y) - \mathcal{E}(X)\mathcal{E}(Y)
    \neq 0
\end{equation*}

Moreover, in an extreme situation of $X=Y$, the covariance equals to the variance.
The second-order moment can be expressed as
\begin{align*}
    \mathcal{E}(X, Y) & = \iint x \cdot x \cdot p(x) \cdot dx dx \\
    \mathcal{E}(X, Y) & = \mathcal{E}(X^2)
\end{align*}
where we use the fact of $p(x, y) = p(x)$ since we have $X=Y$ here.
Using the definition of variance in \eqref{Definition: Expectation and variance definition}, we have
\begin{align*}
    Cov(X, Y) & = \mathcal{E}(X, X) - \mathcal{E}^2(X) \\
    Cov(X, Y) & = \mathcal{E}(X^2) - \mathcal{E}^2(X)  \\
    Cov(X, Y) & = \mathcal{V}
\end{align*}
where $X=Y$.

\bigbreak
\textbf{Variance of mean value}

Without the satisfying situation of $X=Y$, it can be complicated.
However, in a common case, the $Cov(\cdot, \cdot)$ can be used to express the variance of \emph{mean value}
\begin{lemma}
    \label{Lemma: Variance of mean value}
    The variance of mean value equals to the mean of all covariance terms
    \begin{equation*}
        \mathcal{V}(\overline{X}) =
        \frac{1}{n^2} \sum_{i, j} Cov(X_i, X_j),
        i, j \in 1, 2, \dots, n
    \end{equation*}
    no matter the relationship between the statistics of $X_1, X_2, \dots, X_n$.

    \begin{proof}
        The mean value of $n$ statistics can be expressed as
        \begin{equation*}
            \overline{X} = \frac{1}{n} \sum_{i} X_i,
            i \in 1, 2, \dots, n
        \end{equation*}

        Based on the definition, the variance can be expressed as
        \begin{align*}
            Var(\overline{X}) & =  \int_{X_1, X_2, \dots, X_n} {\overline{x}}^2 p(x_1, x_2, \dots, x_n) dx_1 x_2 \dots x_n \\
                              & - (\int_{X_1, X_2, \dots, X_n} \overline{x} p(x_1, x_2, \dots, x_n) dx_1 x_2 \dots x_n)^2  \\
                              & , i, j \in 1, 2, \dots, n
        \end{align*}

        Use the property of full probability rules, we have
        \begin{align*}
            \int_{X_k, \dots} f(x_k) \cdot p(x_k, \dots) dx_k \dots                    & = \int_{X_k} f(x_k) \cdot p(x_k) dx_k                    \\
            \int_{X_i, X_j, \dots} f(x_i, x_j) \cdot p(x_i, x_j, \dots) dx_i x_j \dots & = \int_{X_i, X_j} f(x_i, x_j) \cdot p(x_i, x_j) dx_i x_j
        \end{align*}

        Thus the variance of the mean value can be formulated as
        \begin{align*}
            n^2 \cdot Var(\overline{X}) & = \sum_i \int x_i^2 p(x_i) dx_i + \sum_{i \neq j} \int x_i x_j p(x_i, x_j) dx_i x_j \\
                                        & - (\sum_i \int x_i p(x_i) dx_i)^2
        \end{align*}
        since the positive and negative terms are both of $n^2$ terms.
        By pairing them one-by-one, we come to
        \begin{align*}
            n^2 \cdot Var(\overline{X}) & = \sum_{i, j} \mathcal{E}(X_i X_j) - \mathcal{E}(X_i)\mathcal{E}(X_j) \\
            Var(\overline{X})           & = \frac{1}{n} \sum_{i, j} Cov(X_i, X_j)
        \end{align*}

        Hence proved.
    \end{proof}


\end{lemma}

\subsection{Distributions}
There are several commonly used distributions like: \emph{Normal distribution}, \emph{Binomial distribution}, \emph{Chi-squared distribution}, \emph{Student's t-distribution} and \textit{etc}.

\bigbreak
\textbf{The PDF of Normal distribution is}
\begin{equation}
    \label{Definition: PDF of Normal distribution}
    p(x)=
    \frac{1}{\sqrt{2\pi}\sigma} \exp{\frac{(x-\mu)^2}{2\sigma^2}},
    x \in (-\infty, \infty)
\end{equation}
where $\mathcal{E}=\mu$ and $\mathcal{V}=\sigma^2$.
The normal distribution is so important that we express it as $p(x) \sim \mathcal{N}(\mu, \sigma^2)$.

\bigbreak
\textbf{The PDF of Binomial distribution is}
\begin{equation}
    \label{Definition: PDF of Binomial distribution}
    p_N(n)=
    (N, n) \cdot r^n \cdot (1-r)^{N-n},
    n \in [0, N]
\end{equation}
where $\mathcal{E}=N \cdot r$ and $\mathcal{V}=N \cdot r \cdot (1-r)$.

\bigbreak
\textbf{The PDF of Chi-squared distribution is}
\begin{equation}
    \label{Definition: PDF of Chi-squared distribution}
    p_r(x)=
    \frac{x^{r/2-1} e^{-x/2}}{\Gamma(r/2) 2^{r/2}},
    x \in (0, \infty)
\end{equation}
where $\mathcal{E}=r$ and $\mathcal{V}=2r$.

The statistic follows Chi-squared distribution refers
\begin{equation*}
    p_r(x) \sim \mathcal{X}^2(r) = \sum_{i=1}^{r} Y_i^2
\end{equation*}
where $Y_i \sim \mathcal{N}(0, 1)$, and $Y_i$s are independent with each other.

\subsection{Parameter Estimation}
One goal of statistical analysis is to \emph{determine the parameters of the distribution}.
There are several methods of the estimation:
\begin{itemize}
    \item MLE: Maximum Likelihood Estimation
    \item MAP: Maximum A posteriori Probability estimate
\end{itemize}

\end{document}
